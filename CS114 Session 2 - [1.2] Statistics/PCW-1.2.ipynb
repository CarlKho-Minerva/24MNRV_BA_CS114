{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the formatted content for your Jupyter Notebook. You can copy and paste this directly into your notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '½' (U+00BD) (1769832516.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    time guide: you should allocate approximately 2½ hours to preparing for class, including reading the study guide and doing the pre-class exercises.\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '½' (U+00BD)\n"
     ]
    }
   ],
   "source": [
    "# pre-class workbook\n",
    "cs114. lesson 2. statistics.\n",
    "\n",
    "time guide: you should allocate approximately 2½ hours to preparing for class, including reading the study guide and doing the pre-class exercises.\n",
    "\n",
    "## session video\n",
    "\n",
    "here is the link for today's summary video. if you have any questions, make sure to ask them below.\n",
    "\n",
    "## introduction to statistics\n",
    "\n",
    "from wasserman, l. (2004). all of statistics.\n",
    "\n",
    "> “the basic problem that we study in probability is: given a data generating process, what are the properties of the outcomes?”\n",
    "> “the basic problem of statistical inference is the inverse of probability: given the outcomes, what can we say about the process that generated the data?”\n",
    "\n",
    "in lesson 1, we looked at probability theory as the framework behind data generating processes (also called models or simulations). in this lesson, we look at statistics as the framework behind the inverse process. you will also see examples where we combine probability and statistics – where we identify a plausible model using some observed data, and then make predictions using the plausible model.\n",
    "\n",
    "when applying statistical methods, we generally try to answer one of four types of questions:\n",
    "\n",
    "- hypothesis testing / model comparison: which one of two (or more) simulations generated the observed data? or, given the data, which simulation is the right one?\n",
    "- parameter estimation: what were the inputs to a known simulation that produced the observed data? or, given the data, which input variables were used in the simulation?\n",
    "- prediction: what will future data from the same simulation and with the same input variables look like?\n",
    "- causation: what will happen if we intervene in the simulation and change something about how it works.\n",
    "\n",
    "in this lesson, we focus on the first three questions. the final question (causation) is important and different enough that we devote the next lesson (lesson 3) to it.\n",
    "\n",
    "## sidebar: why do people hate statistics?\n",
    "\n",
    "from james b. ramsey, j.b. (1999). why do students find statistics so difficult? (this short article (4 pages) is recommended reading for everyone but the most important part is below.)\n",
    "\n",
    "> it is a common experience for statisticians when introducing themselves to a non-statistician at a reception to be greeted with the observation; “i took statistics once and hated it; all those formulae to memorise.” the questions are why we get this reaction and what we can do about it. in my answer below i am presuming it is useful for society at large, much less the profession itself, that non-statisticians should have a reasonably sophisticated understanding of statistics and probability. further, it is a depressing fact that corporations, government agencies, and other organisations are staffed with individuals, even those holding ma degrees, who do not understand what it is that they are doing and why. again we should query why the state described is universally true.\n",
    "> [...]\n",
    "> because the student finds the typical exposition so baffling, the student's defence is to rely solely on the memorisation of formulae. the memorised formulae are then inserted into “problems” for which rules of thumb have been developed, often aided by the instructor, to know which formula is to be plugged into which problem. this is not a bad strategy for getting a good grade in the course. however, once the memorisation strategy has been espoused, learning by the student is now entirely lost.\n",
    "\n",
    "statistics is a very opaque field to outsiders (non-statisticians). there are some necessary reasons for this – it requires math which not everyone is comfortable with and it takes time to build good intuition for how probability and statistics work. but, there are some very unnecessary reasons too.\n",
    "\n",
    "- using hard-to-remember technical names makes it difficult to read, never mind understand, statistical texts. for example, calling something bessel’s correction rather than a population vs sample correction for computing standard deviation, or calling something the bonferroni correction rather than a multiple-comparison correction when doing many hypothesis tests, doesn't aid understanding.\n",
    "- memorizing formulas rather than understanding the underlying data generating process makes it difficult to generalize or transfer your knowledge. memorization also gives a false sense of confidence since we might apply a formula and get a good-looking answer without carefully checking whether or not the formula is applicable in the given context.\n",
    "- it seems that there are lots of different ways to do the same thing in statistics – should you use the f-test, t-test, chi-squared test, a particular correction factor, anova, etc.? but, in fact, there are very few types of processes in statistics.\n",
    "\n",
    "we either run a model/simulation forward (probability) or we try to figure out which model/simulation explains our data (statistics). everything else follows from those principles. every statistical test is, at its core, the same thing. it compares two (or more) models/simulations to decide which ones offer a plausible explanation of the data. you will see many examples of this process in the pre-class work.\n",
    "\n",
    "every statistical test is derived in this way (so are p-values) and we get a whole zoo of different tests because there are different types of random variables and different distributions over random variables and because we have to make approximations when the math is too complicated. these different variables or approximations are given names but learning the names directly is almost useless in the long run.\n",
    "\n",
    "okay, enough ranting. how do we do statistics?\n",
    "\n",
    "## question 1 of 10\n",
    "any questions?\n",
    "\n",
    "are there any questions you would like us to discuss during the class? as you go through the readings and exercises in this workbook, return to this point and make a list of questions you would like us to clear up in class.\n",
    "\n",
    "## model comparison\n",
    "\n",
    "- make sure you understand how the model/simulation generates outputs.\n",
    "- look at the data you were given.\n",
    "- which model is more likely to have generated the observed data?\n",
    "\n",
    "this is the essence of model comparison, also known as hypothesis testing. you can compare more than two models this way but, we'll stick to two for today to keep things simple.\n",
    "\n",
    "## required exercise: model comparison #1\n",
    "\n",
    "below are two simulations, called model1 and model2. you can run them as many times as you want or change the number of samples to experiment with the histograms of outputs.\n",
    "\n",
    "run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sts\n",
    "\n",
    "\n",
    "def model1(samples=1000):\n",
    "    return sts.norm(3, 1).rvs(size=samples)\n",
    "\n",
    "\n",
    "def model2(samples=1000):\n",
    "    return sts.gamma(3, scale=1).rvs(size=samples)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"model 1 output distribution\")\n",
    "plt.xlabel(\"model output\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(model1(), bins=np.linspace(0, 10, 30), edgecolor=\"white\")\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"model 2 output distribution\")\n",
    "plt.xlabel(\"model output\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(model2(), bins=np.linspace(0, 10, 30), edgecolor=\"white\")\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [3]\n",
    "\n",
    "here's a data set. run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [3.1656, 3.184, 2.964, 2.741, 1.010, 3.939, 1.794, 2.725, 1.483, 3.484]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"data histogram ({len(data)}) values\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(data, edgecolor=\"white\")\n",
    "plt.xlim(0, 10)\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [4]\n",
    "\n",
    "## question 2 of 10\n",
    "which of the two models above do you think generated the data set? how certain are you? express your answer as a percentage and explain your reasoning. for example, \"i’m 80% certain model a generated the data because ...\" or \"i can’t choose – i’m 50% certain it’s model a and 50% certain it’s model b – because ...\" refer to relevant features of the distribution to explain your conclusion.\n",
    "\n",
    "you're not expected to do any math here. use the shapes of the distributions to support your argument.\n",
    "\n",
    "i am 65% certain that model 1 generated the data. the main reason for this is the shape of the distribution of the data.\n",
    "\n",
    "the data appears to center around the value 3 on the x-axis, which aligns closely with model 1’s output. although the sample size is small (10 values), the distribution remains relatively consistent and clustered around the central value, suggesting that the underlying model is designed to produce results near 3.\n",
    "\n",
    "i chose 65% certainty because, with more iterations, i would expect this distribution to more clearly represent the law of large numbers, where more data points would reinforce the central tendency at 3.\n",
    "\n",
    "on the other hand, model 2’s output seems to center more around the value 2 on the x-axis, which doesn’t match the data as well. while some larger values in the data lean towards model 2, this pattern is less consistent, with only a few values deviating in that direction. if model 2 were the true generator, i would expect the majority of data points to cluster around 2, which is not the case here.\n",
    "\n",
    "## required exercise: model comparison #2: hidden model\n",
    "\n",
    "below are two more models. this time the code cell is hidden but you can still run it as many times as you want to see samples from the two models.\n",
    "\n",
    "run the hidden code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [7]\n",
    "\n",
    "the data set. run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    4.859,\n",
    "    5.092,\n",
    "    5.266,\n",
    "    5.372,\n",
    "    7.200,\n",
    "    5.566,\n",
    "    4.982,\n",
    "    3.650,\n",
    "    2.659,\n",
    "    4.515,\n",
    "    4.705,\n",
    "    4.885,\n",
    "    3.777,\n",
    "    6.074,\n",
    "    7.704,\n",
    "    5.790,\n",
    "    5.629,\n",
    "    4.381,\n",
    "    4.346,\n",
    "    4.458,\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"data histogram ({len(data)}) values\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(data, edgecolor=\"white\")\n",
    "plt.xlim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [8]\n",
    "\n",
    "## question 3 of 10\n",
    "which model generated the data set? how certain are you? express your answer as a percentage and explain your reasoning. refer to relevant features of the distribution to explain your conclusion.\n",
    "\n",
    "i’m 80% certain that neither of the above models created this histogram. my confidence increased because we are working with 20 values, and many of these values cluster around the x-axis value 5. based on this observation, neither model 1 nor model 2 matches the data distribution well.\n",
    "\n",
    "from my visual inspection, model 1 tends to center around output 4, while model 2 appears to center around output 6 on the x-axis. however, in the histogram, most values are grouped around 5, which doesn’t align with either model’s expected output. unless there were extreme deviations in the data, which don’t seem evident here, it’s unlikely that either model produced these results.\n",
    "\n",
    "again, i could be wrong because these might be early stages, where the law of large numbers hasn't applied yet.\n",
    "\n",
    "## required exercise: model comparison #3\n",
    "\n",
    "run the hidden code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [10]\n",
    "\n",
    "the data set. run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    4.526,\n",
    "    5.957,\n",
    "    5.124,\n",
    "    7.232,\n",
    "    4.994,\n",
    "    5.047,\n",
    "    2.400,\n",
    "    4.908,\n",
    "    6.093,\n",
    "    5.038,\n",
    "    4.944,\n",
    "    7.046,\n",
    "    4.412,\n",
    "    5.315,\n",
    "    3.131,\n",
    "    5.719,\n",
    "    5.133,\n",
    "    2.756,\n",
    "    4.942,\n",
    "    3.128,\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"data histogram ({len(data)}) values\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(data, edgecolor=\"white\")\n",
    "plt.xlim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [11]\n",
    "\n",
    "## question 4 of 10\n",
    "which model generated the data set? how certain are you? express your answer as a percentage and explain your reasoning. refer to relevant features of the distribution to explain your conclusion.\n",
    "\n",
    "this one is tricky, but i believe that model 2 generated this histogram. although much of the data sharply clusters around the value 5, which could suggest model 1, there are key details that make me lean toward model 2. model 1 is limited to a narrower range of x-axis values, primarily between 3 and 7, while model 2 covers the entire range from 0 to 10. this wider range better aligns with the spread of data in the histogram.\n",
    "\n",
    "however, i do have some reservations. model 2 seems to distribute the data more evenly between 4 and 5, while model 1 shows a sharper concentration of values around 5. despite this, the broader variety of data points, including values closer to 2, makes me favor model 2.\n",
    "\n",
    "given this balance of factors, i remain uncertain and would say i’m 50% confident that model 2 generated the data.\n",
    "\n",
    "## optional exercise: model comparison #4: stock price models\n",
    "\n",
    "two people want to make money on the stock market. each person thinks they can make money because they have a clever way of predicting stock prices by building a stock price model.\n",
    "\n",
    "their amazing stock price models are given below, along with real stock data (in black on the plot). you can run the cell multiple times to generate more simulated stock prices. you can also change the number_of_samples variable to generate more stock graphs on the same plots.\n",
    "\n",
    "run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    import numpy as np\n",
    "    import scipy.stats as sts\n",
    "\n",
    "    alpha = 6.263e-4\n",
    "    beta = 1.999e-2\n",
    "    start_price = 524.59\n",
    "    n = 250\n",
    "    prices = [start_price]\n",
    "    while len(prices) < n:\n",
    "        prices.append(prices[-1] * np.exp(sts.norm.rvs(loc=alpha, scale=beta)))\n",
    "    return prices\n",
    "\n",
    "\n",
    "def model2():\n",
    "    import numpy as np\n",
    "    import scipy.stats as sts\n",
    "\n",
    "    alpha = 5.331e-4\n",
    "    beta = 1.993e-2\n",
    "    gamma = 5.013e-5\n",
    "    start_price = 524.59\n",
    "    n = 250\n",
    "    prices = [start_price]\n",
    "    vel = alpha\n",
    "    while len(prices) < n + 1:\n",
    "        prices.append(\n",
    "            prices[-1] * np.exp(vel) * np.exp(sts.norm.rvs(loc=0, scale=beta))\n",
    "        )\n",
    "        vel = vel * 0.8 + alpha * 0.2 + sts.norm.rvs(loc=0, scale=gamma)\n",
    "    return prices[1:]\n",
    "\n",
    "\n",
    "data = [\n",
    "    524.59,\n",
    "    540.73,\n",
    "    522.86,\n",
    "    520.8,\n",
    "    500.49,\n",
    "    508.89,\n",
    "    510.4,\n",
    "    499.1,\n",
    "    494.25,\n",
    "    507.79,\n",
    "    500.86,\n",
    "    497.98,\n",
    "    501.77,\n",
    "    586.34,\n",
    "    579.84,\n",
    "    565.17,\n",
    "    556.78,\n",
    "    561.93,\n",
    "    523.28,\n",
    "    538.6,\n",
    "    532.39,\n",
    "    539.04,\n",
    "    548.16,\n",
    "    539.45,\n",
    "    552.16,\n",
    "    550.79,\n",
    "    547.92,\n",
    "    559.07,\n",
    "    563.59,\n",
    "    557.59,\n",
    "    556.52,\n",
    "    557.28,\n",
    "    551.34,\n",
    "    548.22,\n",
    "    540.22,\n",
    "    533.78,\n",
    "    546.15,\n",
    "    553.41,\n",
    "    546.7,\n",
    "    538.85,\n",
    "    550.64,\n",
    "    547.82,\n",
    "    520.7,\n",
    "    511.29,\n",
    "    516.39,\n",
    "    493.33,\n",
    "    506.44,\n",
    "    504.54,\n",
    "    523.06,\n",
    "    518.02,\n",
    "    520.25,\n",
    "    524.03,\n",
    "    524.44,\n",
    "    504.79,\n",
    "    512.18,\n",
    "    523.11,\n",
    "    535.09,\n",
    "    520.81,\n",
    "    502.86,\n",
    "    508.05,\n",
    "    513.95,\n",
    "    513.39,\n",
    "    521.66,\n",
    "    539.42,\n",
    "    540.67,\n",
    "    544.53,\n",
    "    546.99,\n",
    "    554.58,\n",
    "    555.31,\n",
    "    552.78,\n",
    "    553.73,\n",
    "    540.02,\n",
    "    549.22,\n",
    "    546.54,\n",
    "    554.44,\n",
    "    549.57,\n",
    "    508.9,\n",
    "    508.78,\n",
    "    505.55,\n",
    "    510.3,\n",
    "    505.55,\n",
    "    506.52,\n",
    "    509,\n",
    "    513.47,\n",
    "    509.11,\n",
    "    503.18,\n",
    "    496.08,\n",
    "    499.55,\n",
    "    503.84,\n",
    "    486.69,\n",
    "    495.08,\n",
    "    484.98,\n",
    "    486.66,\n",
    "    493.37,\n",
    "    488.94,\n",
    "    486.28,\n",
    "    487.7,\n",
    "    501.67,\n",
    "    497.89,\n",
    "    502.9,\n",
    "    501.34,\n",
    "    502.36,\n",
    "    503.86,\n",
    "    502.81,\n",
    "    499.08,\n",
    "    499.24,\n",
    "    489.43,\n",
    "    494.74,\n",
    "    494.66,\n",
    "    492.39,\n",
    "    485.81,\n",
    "    487.27,\n",
    "    488.77,\n",
    "    499.89,\n",
    "    491.9,\n",
    "    492.41,\n",
    "    498.34,\n",
    "    500.77,\n",
    "    497,\n",
    "    508.82,\n",
    "    512.74,\n",
    "    518.06,\n",
    "    527.07,\n",
    "    533.03,\n",
    "    533.5,\n",
    "    528.21,\n",
    "    533.54,\n",
    "    533.98,\n",
    "    541.64,\n",
    "    535.96,\n",
    "    530.76,\n",
    "    535.98,\n",
    "    537.31,\n",
    "    540.68,\n",
    "    547.95,\n",
    "    542.95,\n",
    "    530.31,\n",
    "    532.28,\n",
    "    531.05,\n",
    "    513.63,\n",
    "    511.77,\n",
    "    515.41,\n",
    "    516.49,\n",
    "    518.91,\n",
    "    519.3,\n",
    "    514.25,\n",
    "    517.57,\n",
    "    515.15,\n",
    "    510.82,\n",
    "    517.35,\n",
    "    524.89,\n",
    "    520.55,\n",
    "    519.97,\n",
    "    515.84,\n",
    "    512.4,\n",
    "    510.72,\n",
    "    515.92,\n",
    "    517.92,\n",
    "    518.91,\n",
    "    521.87,\n",
    "    543.71,\n",
    "    546.88,\n",
    "    553.33,\n",
    "    553.41,\n",
    "    547.58,\n",
    "    550.12,\n",
    "    558.92,\n",
    "    566.18,\n",
    "    569.19,\n",
    "    582.07,\n",
    "    588.55,\n",
    "    590.53,\n",
    "    606.71,\n",
    "    606.05,\n",
    "    597.54,\n",
    "    598.72,\n",
    "    589.29,\n",
    "    577.76,\n",
    "    582.87,\n",
    "    586.5,\n",
    "    589.35,\n",
    "    575.43,\n",
    "    573.14,\n",
    "    590.65,\n",
    "    593.26,\n",
    "    592.39,\n",
    "    592.64,\n",
    "    583.85,\n",
    "    599.06,\n",
    "    610.34,\n",
    "    613.15,\n",
    "    603.35,\n",
    "    634.81,\n",
    "    639.1,\n",
    "    631.85,\n",
    "    632.66,\n",
    "    627.04,\n",
    "    624.94,\n",
    "    629.76,\n",
    "    633.8,\n",
    "    628.29,\n",
    "    637.97,\n",
    "    639,\n",
    "    625.14,\n",
    "    653.16,\n",
    "    664.78,\n",
    "    671.66,\n",
    "    668.52,\n",
    "    662.92,\n",
    "    674.05,\n",
    "    690.31,\n",
    "    681.17,\n",
    "    677.72,\n",
    "    688.29,\n",
    "    668.4,\n",
    "    645.72,\n",
    "    651.45,\n",
    "    655.99,\n",
    "    646.91,\n",
    "    657.58,\n",
    "    682.61,\n",
    "    679.33,\n",
    "    687.4,\n",
    "    691.69,\n",
    "    682.02,\n",
    "    678.8,\n",
    "    659.2,\n",
    "    654.06,\n",
    "    658.29,\n",
    "    665.64,\n",
    "    663.84,\n",
    "    641.9,\n",
    "    617.77,\n",
    "    616.47,\n",
    "    602.13,\n",
    "    612.69,\n",
    "    625.58,\n",
    "    628.08,\n",
    "    611,\n",
    "    611.66,\n",
    "    604.56,\n",
    "    597.99,\n",
    "    605.04,\n",
    "    591.06,\n",
    "    586.73,\n",
    "    593.74,\n",
    "    604.92,\n",
    "    614.24,\n",
    "    614.09,\n",
    "    613.12,\n",
    "]\n",
    "\n",
    "number_of_samples = 100\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(f\"Model 1 simulated stock price ({number_of_samples} samples)\")\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"price ($)\")\n",
    "for _ in range(number_of_samples):\n",
    "    plt.plot(model1(), alpha=0.5)\n",
    "plt.plot(data, \"k\", label=\"real data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(f\"Model 2 simulated stock price ({number_of_samples} samples)\")\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"price ($)\")\n",
    "for _ in range(number_of_samples):\n",
    "    plt.plot(model2(), alpha=0.5)\n",
    "plt.plot(data, \"k\", label=\"real data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "question 5 of 10\n",
    "which model generates data that looks most like the real data? how certain are you? express your answer as a percentage and explain your reasoning. refer to relevant features of the distribution to explain your conclusion.\n",
    "\n",
    "i am 75% certain that model 2 is better for predicting stock prices. when working with smaller sample sizes, i noticed fewer discrepancies between the simulated lines and the real data. however, as i increased the sample size to hundreds, model 1 showed more significant outliers, and in simulations with 20 samples, the lines from model 1 intersected the real data more frequently. over the long run, this led to a greater variance. in contrast, model 2 consistently produced simulations that were closer to the real stock data, with less variance over time. this tighter alignment with the real data makes me lean towards model 2 as the more accurate model.\n",
    "\n",
    "🤨\n",
    "\n",
    "parameter estimation\n",
    "\n",
    "if we think we know what the data generation process (simulation) is, we might still need to find the parameters of the model (the inputs of the simulation). the output distribution of a simulation can potentially vary a lot when the inputs are changed.\n",
    "\n",
    "the basic process for estimating the value of an input parameter is very similar to comparing two different models/simulations.\n",
    "\n",
    "make sure you understand how the model/simulation generates outputs.\n",
    "look at the data you were given.\n",
    "which settings of the input parameter(s) are plausible given that the simulation generated the observed data?\n",
    "\n",
    "the exercises below are intended to give you a sense of how easy or hard it is to figure out which simulation inputs lead to which simulation outputs. in the exercises below, there are between 1 and 3 input parameters. so, to find a good setting of the parameters, you need to explore a 1-dimensional space or a 3-dimensional space.\n",
    "\n",
    "in practice, we have to be able to handle such small parameter spaces but also much larger ones. modern machine learning models (like the ones you encounter in cs156), to take an extreme example, can have billion-dimensional parameter spaces. we need specialized statistical techniques to be able to find anything meaningful in such large spaces. how this is done is the topic of another whole course, namely cs146.\n",
    "\n",
    "for now, we'll keep it simple.\n",
    "\n",
    "required exercise: parameter estimation #1\n",
    "\n",
    "in this hidden cell is a function that runs a simulation and generates an output. run it multiple times and plot a histogram of its outputs. try out different values of the input parameter.\n",
    "\n",
    "run the hidden code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make a histogram\n",
    "plt.figure()\n",
    "samples = [model1(6) for _ in range(50)]\n",
    "plt.hist(samples)\n",
    "plt.show()\n",
    "\n",
    "# task: the code above makes a histogram for the input parameter 10. try other\n",
    "#       values of the input parameter and observe how the histogram changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [56]\n",
    "\n",
    "here is a data set generated by this model. run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    2.045558,\n",
    "    3.099034,\n",
    "    2.656915,\n",
    "    1.679386,\n",
    "    1.736054,\n",
    "    2.159028,\n",
    "    1.745616,\n",
    "    2.296223,\n",
    "    1.845166,\n",
    "    1.769607,\n",
    "    2.437620,\n",
    "    1.744182,\n",
    "    1.727201,\n",
    "    3.411810,\n",
    "    1.602046,\n",
    "    1.791977,\n",
    "    2.261734,\n",
    "    2.173848,\n",
    "    2.115976,\n",
    "    3.001945,\n",
    "    1.248749,\n",
    "    1.698229,\n",
    "    1.663228,\n",
    "    1.982264,\n",
    "    2.660400,\n",
    "    1.606541,\n",
    "    2.613060,\n",
    "    3.051579,\n",
    "    2.825520,\n",
    "    2.050120,\n",
    "    2.171277,\n",
    "    2.715401,\n",
    "    2.744453,\n",
    "    2.341670,\n",
    "    1.655787,\n",
    "    2.540851,\n",
    "    2.563441,\n",
    "    2.402588,\n",
    "    2.407787,\n",
    "    1.516844,\n",
    "    1.902128,\n",
    "    2.756363,\n",
    "    1.535858,\n",
    "    2.171033,\n",
    "    1.946100,\n",
    "    2.669550,\n",
    "    1.792815,\n",
    "    1.674384,\n",
    "    1.498777,\n",
    "    2.280294,\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"data histogram\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.hist(data, edgecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "out [37]\n",
    "\n",
    "question 6 of 10\n",
    "what is your best estimate for the input parameter that generated the observed output? how certain are you? give a range of plausible values to express your certainty. for example, \"i think the input parameter is between 1 and 3 because ...\"\n",
    "\n",
    "you should try out many different values of the model input and try to zoom in on the range of values that seem reasonable. you are not expected to do any math.\n",
    "\n",
    "required exercise: parameter estimation #2\n",
    "\n",
    "here is another model with one input parameter and a data set. make a histogram of the data and the model output and estimate the input parameter.\n",
    "\n",
    "run the hidden code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model2(10)\n",
    "print(output)\n",
    "\n",
    "data = [\n",
    "    0.196,\n",
    "    0.715,\n",
    "    0.650,\n",
    "    0.564,\n",
    "    0.585,\n",
    "    0.702,\n",
    "    0.329,\n",
    "    0.331,\n",
    "    0.356,\n",
    "    0.588,\n",
    "    0.446,\n",
    "    0.197,\n",
    "    0.644,\n",
    "    0.683,\n",
    "    0.541,\n",
    "    0.698,\n",
    "    0.258,\n",
    "    0.474,\n",
    "    0.613,\n",
    "    0.457,\n",
    "    0.373,\n",
    "    0.433,\n",
    "    0.348,\n",
    "    0.677,\n",
    "    0.289,\n",
    "    0.488,\n",
    "    0.634,\n",
    "    0.536,\n",
    "    0.693,\n",
    "    0.447,\n",
    "    0.530,\n",
    "    0.850,\n",
    "    0.864,\n",
    "    0.697,\n",
    "    0.598,\n",
    "    0.560,\n",
    "    0.763,\n",
    "    0.133,\n",
    "    0.552,\n",
    "    0.673,\n",
    "    0.528,\n",
    "    0.482,\n",
    "    0.380,\n",
    "    0.666,\n",
    "    0.826,\n",
    "    0.691,\n",
    "    0.408,\n",
    "    0.725,\n",
    "    0.497,\n",
    "    0.324,\n",
    "]\n",
    "\n",
    "# task: make a histogram. try out different values of the input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "question 7 of 10\n",
    "what is your best estimate for the input parameter that generated the observed output? how certain are you? give a range of plausible values to express your certainty.\n",
    "\n",
    "required exercise: parameter estimation #3: using statistics\n",
    "\n",
    "this time, we'll use two statistics of the data to estimate the input parameter of a model. rather than making a histogram of the data, we compute the mean and standard deviation of the data (see the code below).\n",
    "\n",
    "next, we produce some outputs from the model and compute the mean and standard deviation of those outputs.\n",
    "\n",
    "try to find the input parameter value that matches the data mean and standard deviation as closely as you can.\n",
    "\n",
    "run the hidden code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# print out statistics of the data\n",
    "\n",
    "data = [\n",
    "    13.229,\n",
    "    17.306,\n",
    "    8.775,\n",
    "    11.357,\n",
    "    7.815,\n",
    "    10.400,\n",
    "    10.340,\n",
    "    9.230,\n",
    "    8.375,\n",
    "    9.384,\n",
    "    13.715,\n",
    "    11.640,\n",
    "    8.109,\n",
    "    10.137,\n",
    "    10.653,\n",
    "    10.332,\n",
    "    9.221,\n",
    "    10.166,\n",
    "    11.187,\n",
    "    9.647,\n",
    "    11.887,\n",
    "    15.658,\n",
    "    12.063,\n",
    "    16.320,\n",
    "    8.571,\n",
    "    14.595,\n",
    "    8.071,\n",
    "    10.472,\n",
    "    11.279,\n",
    "    9.731,\n",
    "    7.194,\n",
    "    9.666,\n",
    "    6.001,\n",
    "    9.312,\n",
    "    12.430,\n",
    "    10.601,\n",
    "    13.810,\n",
    "    13.172,\n",
    "    9.775,\n",
    "    13.483,\n",
    "    18.820,\n",
    "    11.433,\n",
    "    12.122,\n",
    "    8.322,\n",
    "    9.985,\n",
    "    8.180,\n",
    "    9.509,\n",
    "    10.035,\n",
    "    12.532,\n",
    "    11.490,\n",
    "]\n",
    "\n",
    "print(\"data mean:\", np.mean(data))\n",
    "print(\"data standard deviation:\", np.std(data))\n",
    "print()\n",
    "\n",
    "# print out statistics of the model output\n",
    "\n",
    "input_parameter = 3\n",
    "model_outputs = [model3(input_parameter) for _ in range(50)]\n",
    "\n",
    "print(\"input parameter value:\", input_parameter)\n",
    "print(\"simulated mean:\", np.mean(model_outputs))\n",
    "print(\"simulated standard deviation:\", np.std(model_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "run code\n",
    "question 8 of 10\n",
    "what is your best estimate for the input parameter that generated the observed output? how certain are you? give a range of plausible values to express your certainty.\n",
    "\n",
    "required exercise: parameter estimation #4: the universe\n",
    "\n",
    "estimate the input parameters of the universe – specifically, this cosmic microwave background radiation simulation. try to get the \"universe similarity\" metric (bottom-right corner) close to 100%.\n",
    "\n",
    "this simulation has 3 parameters so you will need to hunt around in the 3-dimensional parameter space to find a good setting of the parameters. what process can you follow to find a good setting of the input parameters?\n",
    "\n",
    "this simulation is based on a model of the evolution of the early universe. the data for the comparison came from the planck space observatory.\n",
    "\n",
    "question 9 of 10\n",
    "(a) record the input parameters of your best results.\n",
    "\n",
    "question 10 of 10\n",
    "(b) describe the process you followed to find the best setting of the parameters. can you use a similar process to do parameter estimation in other models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
